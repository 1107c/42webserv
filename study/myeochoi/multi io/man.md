1. 하나의 서버에서 여러개의 클라이언트 처리하기 위한 방법은 크게 세가지가 있다.
	a. 멀티 프로세싱
		- 우리가 이전에 배워왔듯이 fork 함수를 통해 소켓을 복제하는것이다. 각 프로세스가 복제된 소켓을 통해 통신함으로 동기적으로 통신할 수 있지만 프로세스 복제에 대한 비용이 크다.
	b. 멀티 쓰레딩
		- 쓰레드는 프로세스에 비해서 비용이 적지만 쓰레드 프로그래밍의 큰 단점은 디버깅이 어렵고 안전한 코드를 작성하기 위한 난이도가 높다. 또한 쓰레드는 프로세스 내부에서 자원을 공유하기 때문에 완벽한 프로그래밍을 하지 않을 경우 하나의 쓰레드에서 발생한 에러의 범위가 전체 쓰레드로 확장되는 리스크가 크다.
	c. i/o 멀티 플렉싱
		- 기본적으로 소켓통신은 블로킹 모드이다. 블로킹 모드는 클라이언트 하나에서 응답이 올 때 까지 서버소켓이 블로킹 된다. 그에 반해 논블로킹 모드는 클라이언트를 순회하며 응답이 오는것인지 지속적으로 확인하는 방식이다.
		
		i/o 멀티 플렉싱 입출력 다중화는 블로킹/논블로킹 모드에 상관없이 사용할 수 있다. 입출력 다중화는 여러 클라이언트의 소켓 fd, 즉 관심있는 파일디스크립터를 효율적으로 관리하는 함수이기 때문이다. 하지만 논블로킹 모드와 결합되어 사용할 경우 입출력 다중화의 소기의 목적인 파일 디스크립터의 효율적 관리를 달성할 수 있다.

		i.	select
			int select(int nfds, fd_set *restrict readfds,
			fd_set *restrict writefds, fd_set *restrict exceptfds,
			struct timeval *restrict timeout);
			select의 동작 방식
				입출력 다중화의 유닉스 계열 가장 태초 함수인 select는 이름 그대로 fd를 선택하는 함수이다.
				입출력 다중화의 동작 방식은 관리하고자 하는 fd들을 각 함수들의 자료구조에 따라 관리한다. select의 경우 1024개(보통 1024개로 define 되어 있다. fd_setsize.c를 컴파일 하여 확인해 볼 수 있음)의 비트 배열로 fd를 관리한다.

				최초에 FD_ZERO 호출 시 가지고 있는 비트 배열을 모두 0으로 초기화한다. 그 다음 관리하고자 하는 fd들을 FD_SET 호출 시 등록한다 (1로 만든다). 이후 select 호출 시 0부터 첫번째 인자인 nfds(1로 설정된 파일 디스크립터의 최대값 + 1)까지 순회를 하면서 해당 소켓 디스크립터에 변화(클라이언트의 응답)가 있는지 검사하고 변화가 있다면 1로 유지하고 나머지는 0으로 설정한다. 검사 이후 변화가 있는 fd의 총 갯수를 리턴한다. 이후 FD_ISSET 매크로를 통해 집합에 있는 파일디스크립터를 확인할 수 있다.

				중간 인자인 readfds, writefds, expectfds에서 위의 검사 알고리즘을 통해 각각 입력받은 초기화한 파일 디스크립터 집합에 대해 동시에 추가적으로 읽기 준비 상태(read가 가능), writefds는 쓰기가능한지 체크를 하며 expectfds는 예외상황(긴급 데이터, 오류 등)인지 살펴본다.
				q. 그러면 3개의 fd_set을 모두 동일하게 전달하면 어떻게 되는가?
				a > 어떤 이벤트로 인해 fd가 설정되었는지 알 수가 없다.

				마지막인자인 timeout은 timeval 구조체(철학자에서 사용했던)이며 5초로 설정한다면 5초동안 파일테이블을 감시하며 클라이언트에서 이벤트가 일어나는것을 대기했다가 파일테이블에 대해 검사를 실행한다.
				이러한 특성때문에 usleep이 등장하기전 select를 이용하여 모든 인자를 0으로 설정하고 timeout으로만 select를 호출하는 방법을 사용햇다.

				timeout 포인터 자체를 null로 할경우 하나라도 응답이 있을 때 까지 무한대기하며 구조체 내부의 변수들을 0으로 할 경우 즉시 검사한다.
			select의 단점
				1. 결국 select가 한번일어난 후 fd_set은 그 시점에서 준비 상태인 fd로 설정되기때문에 select를 호출 할 때마다 매번 fd_set을 초기화 해야한다.
				2. 시간복잡도 측면에서 nfds를 기준으로 0부터 모든 fd를 순회함으로 관심 fd가 아닌것도 강제적으로 순회해야한다. 이것은 해당 함수를 호출할때, 테이블에서 관심 있는 fd를 삭제할때, 리턴에서 실제 변경된 fd를 처리할 때 모두 동일하게 적용된다.
				3. FD_SETSIZE(리눅스에서 보통 1024)보다 큰 fd는 무시된다.
				4. 사용자가 파일 디스크립터 테이블을 수동으로 관리해야하고 nfds도 수동으로 관리한다.
				>> poll 및 epoll의 등장 배경
			select 버그
				select로 감시 중인 파일 디스크립터를 다른 스레드에 닫는 경우의 결과는 명시되어 있지 않으며 유닉스 시스템에 따라 동작이 다르다.

				현재 구현에서는 프로세스가 열어 둔 파일 디스크립터 번호의 최대값 보다 큰 파일 디스크립터를 무시한다(1024 초과 무시).
				하지만 posix 표준에 따르면 EBADF 오류가 발생해야함으로 엄격한 posix를 따르는 시스템과 호환문제가 발생할 수 있다.

				리눅스에서는 select가 어떤 소켓 파일디스크립터를 읽기 준비됨으로 보고하였는데 이후 select가 블록될 수 있다. 예를 들어 데이터가 도착한후 체크섬을 비교해보았을 때 틀려서 폐기할 경우가 있다. 이 경우 오류가 발생하거나, 연결이 종료되거나, 새로운 데이터가 도착할 때까지 해당 소켓이 블록됨으로 블록해서는 안되는 소켓이 있다면 논블록으로 설정하는것이 낫다.

				리눅스에서는 시그널에 의해 호출이 중단된 경우 남은 시간이 얼마인지 나타내기 위해 select에서 timeout을 변경하는데 이것은 posix에서 허용되지 않은 동작이다.
		ii. poll
			int poll(struct pollfd *fds, nfds_t nfds, int timeout);
			poll의 동작 방식
			poll은 select가 관리하지 않는 파일디스크립터도 순회하는것는 문제를 보완하기 위해 등장하였다. poll은 감시하고자 하는 파일 디스크립터에 대한 정보를 구조체 배열로 관리한다.
				struct pollfd {
					int   fd;         /* 파일 디스크립터 */
					short events;     /* 요청한 이벤트 */
					short revents;    /* 반환된 이벤트 */
				};
			예를 들어 events를 POLLIN 플래그로 설정하면 해당fd가 읽기 가능한지 체크하며 다른 플래그들도 그의 목적에 맞게 동작한다.
			revents에서 실제로 읽기 가능한 상태라면 POLLIN을 설정하게 된다.
			그 외에 select와 동일하다.
			poll은 select에 비해서 관리하지 않는 fd까지 순회해야하는 작업은 줄였지만 여전히 관리해야 하는 fd가 많다면 실제로 클라이언트에서 변화가 일어나지 않은 fd까지 순회를 한다는 점이 단점으로 남아있었으며 실제로 관리하는 fd 중 변화가 있는 fd만 처리하는 epoll이 등장하게 되었다.
		iii. epoll
			epoll의 동작 방식
			1. int epfd = epoll_create1(0);
				epoll 인스턴스를 생성한다.
				epoll 파일 디스크립터 즉 epoll 레드블랙트리, 큐 및 메타데이터 등을 가르키는 파일디스크립터를 반환한다.
			2. epoll_ctl(epfd, EPOLL_CTL_ADD, client_sock, &ev);
				감시할 fd를 등록한다.
			3. int event_count = epoll_wait(epfd, events, MAX_EVENTS, timeout);
				이벤트 발생을 감시한다.
			4. 이벤트를 처리한다.
			epoll api의 핵심은 epoll 인스턴스라는 커널 자료구조이며 이것은 관심목록(프로세스에서 감시 대상으로 등록한 파일 디스크립터의 집합), 준비목록(변화가 일어난 파일 디스크립터들의 집합)으로 이루어져 있다.
			관심목록은 레드블랙트리로 이루어져 있으며 준비목록은 큐로 이루어져있다.
			최초에 관심 fd를 레드블랙트리에서 관리하다가 변화가 생긴 fd를 큐에 넣어주는 방식이다.
			이러한 자료구조의 특성을 이용하여 poll, select들의 단점을 극복하였다. 관심목록에 fd를 삭제/추가도 레드블랙트리에서 간단하게 할 수 있으며 데이터를 순회할 때도 준비된 큐에서 fd만 가져오면 됨으로 O(N)이 아닌 O(1)의 시간 복잡도를 가지게 된다.

			레벨 트리거와 에지 트리거
			기본적으로 epoll 인터페이스는 에지 트리거 또는 레벨 트리거로 동작할 수 있다.
			에지 트리거로 사용하는 방법은 ev.events = EPOLLIN | EPOLLET; 
			epoll.events 구조체를 EPOLLET으로 설정하면 된다.
			두 매커니즘의 차이는 아래예시를 통해 비교할 수 있다.
				1. 파이프의 읽기 쪽을 나타내는 파일 디스크립터(rfd)를 epoll 인스턴스에 등록한다.
				2. 파이프 쓰기 쪽에서 2 kB 데이터를 써넣는다.
				3. epoll_wait(2) 호출이 이뤄지고, 준비 상태인 파일 디스크립터로 rfd를 반환한다.
				4. 파이프 읽기 쪽에서 rfd로부터 1 kB 데이터를 읽는다.
				5. epoll_wait(2) 호출이 이뤄진다.
			엣지 트리거 사용 시 5번 단계에서 호출을 중단한다. 엣지 트리거는 실제 감시 디스크립터에서 변화가 발생할 때만 동작하기 때문이다. 2번에서 감시 디스크립터에 변화가 발생하고 그것을 3번에서 소모한다. 4번에서 1kb를 읽었지만 여전히 1kb가 버퍼에 남아있다. 이후에는 해당 디스크립터에 대한 새로운 상태 변화가 없기 때문에 에지 트리거는 5번에서 블록된다. 따라서 EPOLLET 플래그를 사용하면 반드시 논블로킹 모드로 설정하여 데이터가 있으면 계속 읽거나 써서 EAGAIN(버퍼에 데이터 없음)을 반환하게 해야 한다.

			또한 레벨 트리거 epoll은 그냥 더 빠른 poll이며 같은 동작 방식(상태 변화가 있으면 계속 wait 반환, 엣지 트리거는 상태 변화 시 한번 반환, 논블록은 모든 데이터를 읽기 위한 설정)을 공유하기 때문에 poll을 쓰는 곳 어디에든 쓸 수 있다.

			에지 트리거 epoll을 사용해도 감시 디스크립터의 상태가 변경 될때마다 이벤트가 생성될 수 있다. 따라서 EPOLLONESHOT 플래그를 지정하면 한번 수신한다음 해당 파일 디스크립터를 비활성화할 수도 있다. 이후에 EPOLL_CTL_MOD 플래그로 재활성화할 수도 있다. 즉 재활성화 하기 전까지 wait 리턴에서 해당 디스크립터가 제외되며 버퍼에 데이터가 쌓인다. 